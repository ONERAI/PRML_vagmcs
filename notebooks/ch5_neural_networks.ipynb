{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9c3bb4",
   "metadata": {},
   "source": [
    "# 5. Neural Networks\n",
    "\n",
    "### *Table of Contents*\n",
    "5.1 [Feed-forward Network Functions](#5.1-Feed-forward-Network-Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6857fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed to make deterministic\n",
    "np.random.seed(0)\n",
    "\n",
    "# Ignore zero divisions and computation involving NaN values.\n",
    "np.seterr(divide = 'ignore', invalid='ignore')\n",
    "\n",
    "# Enable higher resolution plots\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Enable autoreload all modules before executing code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60247b1",
   "metadata": {},
   "source": [
    "## 5.1 Feed-forward Network Functions\n",
    "\n",
    "The linear models discussed in previous chapters are based on linear combinations of fixed (non)linear basis functions $\\phi_j(\\mathbf{x})$ and take the form\n",
    "\n",
    "$$\n",
    "y(\\mathbf{x},\\mathbf{w}) = f\\Bigg(\\sum_{j=1}^M w_j\\phi_j(\\mathbf{x})\\Bigg)\n",
    "$$\n",
    "\n",
    "where $f(\\cdot)$ is a nonlinear activation function in the case of classification and the identity in the case of regression. Although such models have useful analytical properties, they are limited by the curse of dimentionality, and they need to adapt the basis functions to the data for large-scale problems. An alternative is to use a predefined number of basis functions but allow them to be adaptive during training. Thus, our goal is to extend the model above by making the basis functions $\\phi_j(\\mathbf{x})$ depend on parameters and then adjust them along the coefficients $\\{w_j\\}$, during training.\n",
    "\n",
    "Neural networks use basis functions that follow the same form, that is, each basis function is itself a nonlinear function of a linear combination of the inputs, where the coefficients are adapative parameters. Thus, the basic neural network model is described as a series of functional transformations.\n",
    "\n",
    "1. Given the input variables $x_1,\\dots,x_D$, we construct $M$ linear combinations in the form:\n",
    "\n",
    "$$\n",
    "a_j = \\sum_{i=1}^D w_{ji}^{(1)}x_i + w_{j0}^{(1)}\n",
    "$$\n",
    "\n",
    "where $j=1,\\dots,M$, and the superscript $(1)$ indicates the corresponding parameters of the *first* layer of the network. The quantities $a_j$ are known as activations and each of them is transformed using a *differentiable* nonlinear activation function $h(\\cdot)$ to give\n",
    "\n",
    "$$\n",
    "z_j = h(a_j)\n",
    "$$\n",
    "\n",
    "these correspond to the outputs of the basis functions, and in the context of neural networks are called *hidden units*.\n",
    "\n",
    "2. Following the same procedure, these output values from the *first* layer, are linearly combined again to give,\n",
    "\n",
    "$$\n",
    "a_k = \\sum_{j=1}^M w_{ki}^{(2)}z_j + w_{k0}^{(2)}\n",
    "$$\n",
    "\n",
    "where $k=1,\\dots,K$. This transformation corresponds to the *second* layer of the network. These output activations are transformed again using an appropriate activation function $h$ to give a set of outputs $y_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d119203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
