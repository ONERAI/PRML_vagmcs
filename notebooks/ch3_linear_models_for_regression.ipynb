{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Linear Models for Regression\n",
    "\n",
    "### *Table of Contents*\n",
    "\n",
    "* 3.1 [Linear Basis Function Models](#3.1-Linear-Basis-Function-Models)\n",
    "    * 3.1.1 [Maximum likelihood and least squares](#3.1.1-Maximum-likelihood-and-least-squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed to make deterministic\n",
    "np.random.seed(0)\n",
    "\n",
    "# Ignore zero divisions and computation involving NaN values.\n",
    "np.seterr(divide = 'ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of regression is to predict the value of one or more **continuous** target variables $t$ given the value of a $D$-dimensional vector $\\mathbf{x}$ of input variables. The polynomial curve that we used in [Chapter 1](ch1_introduction.ipynb) belongs to a broader class of functions called linear regression models,\n",
    "that are linear functions of the adjustable parameters. The simplest form of linear regression models are also linear functions of the input variables $\\mathbf{x}$. However, a much more useful class of functions is the linear combinations of a fixed set of nonlinear functions of the input variables, known as *basis functions*.\n",
    "\n",
    "## 3.1 Linear Basis Function Models\n",
    "\n",
    "The simplest linear model for regression is one that involves a linear combination of the input variables\n",
    "\n",
    "$$\n",
    "y(\\mathbf{x}, \\mathbf{w}) = w_0 + w_1x_1 + \\dots + w_Dx_D\n",
    "$$\n",
    "\n",
    "which is simply known as *linear regression*. This model is a linear function of the parameters and a linear function of the input variables, and this imposes significant limitations on the model. We therefore extend the class of models by considering linear combinations of fixed nonlinear functions of the input variables, of the form\n",
    "\n",
    "$$\n",
    "y(\\mathbf{x}, \\mathbf{w}) = w_0 + \\sum_{j=1}^{M-1} w_j\\phi_j(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "where $\\phi_j(\\mathbf{x})$ are known as *basis functions*.\n",
    "\n",
    "The polynomial regression considered in [Chapter 1](ch1_introduction.ipynb) is an example of this model in which there is a single input variable $x$, and the basis functions take the form of powers of $x$ so that $\\phi_j(x)=x^j$.\n",
    "\n",
    "There are many possible choices for the basis functions, for example\n",
    "\n",
    "$$\n",
    "\\phi_j(x) = \\exp\\bigg\\{ \\frac{(x - \\mu_j)^2}{2s^2} \\bigg\\}\n",
    "$$\n",
    "\n",
    "are referred to as *Gaussian* basis functions, where $\\mu_j$ govern the locations of the functions in input space, and $s$ governs their spatial scale.\n",
    "\n",
    "Another possibility is the sigmoidal basis function of the form\n",
    "\n",
    "$$\n",
    "\\phi_j(x) = \\sigma\\bigg( \\frac{x - \\mu_j}{s} \\bigg)\n",
    "$$\n",
    "\n",
    "where $\\sigma(a)$ is the logistic sigmoid function defined by\n",
    "\n",
    "$$\n",
    "\\sigma(a) = \\frac{1}{1 + \\exp(-a)}\n",
    "$$\n",
    "\n",
    "### 3.1.1 Maximum likelihood and least squares\n",
    "\n",
    "In Chapter 1, we fitted polynomial functions to data sets by minimizing a sum-of-squares error function. We also showed that this error function could be motivated as the maximum likelihood solution under an assumed Gaussian noise model. Let us return to the discussion of [Chapter 1](ch1_introduction.ipynb) and consider the least squares approach, and its relation to maximum likelihood, in more detail.\n",
    "\n",
    "As before, we assume that the target variable $t$ is given by a deterministic function $y(\\mathbf{x},\\mathbf{w})$ having additive Gaussian noise so that\n",
    "\n",
    "$$\n",
    "t = y(\\mathbf{x},\\mathbf{w}) + \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a zero mean Gaussian random variable with precision (inverse variance) $\\beta$. Thus we can write\n",
    "\n",
    "$$\n",
    "p(t|\\mathbf{x},\\mathbf{w},\\beta) = \\mathcal{N}(t|y(\\mathbf{x},\\mathbf{w}),\\beta^{-1})\n",
    "$$\n",
    "\n",
    "Note that the Gaussian noise assumption implies that the conditional distribution of $t$ given $\\mathbf{x}$ is unimodal, which may be inappropriate for some applications.\n",
    "\n",
    "Now consider a data set of inputs $\\mathbf{X}=\\{\\mathbf{x}_1,\\dots,\\mathbf{x}_N\\}$ along corresponding target alues $\\mathsf{t}=(t_1,\\dots,t_N)^{\\text{T}}$. Assuming that the data points are i.i.d, we obtain the likelihood function (function of the adjustable parameters $\\mathbf{w}$ and $\\beta$), in the form\n",
    "\n",
    "$$\n",
    "p(\\mathsf{t}|\\mathbf{X},\\mathbf{w},\\beta) = \n",
    "\\prod_{n=1}^N \\mathcal{N}(t_n|y(\\mathbf{x}_n,\\mathbf{w}),\\beta^{-1}) = \n",
    "\\prod_{n=1}^N \\mathcal{N}(t_n|\\mathbf{w}^{\\text{T}}\\boldsymbol\\phi(\\mathbf{x}_n),\\beta^{-1})\n",
    "$$\n",
    "\n",
    "**NOTE**: In many textbooks, the input variables $\\mathbf{x}$ are dropped from the set of conditioning variables, since, we do not seek to model the distribution of $\\mathbf{x}$.\n",
    "\n",
    "Taking the logarithm of the likelihood function, we have,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ln p(\\mathsf{t}|\\mathbf{X},\\mathbf{w},\\beta) &= \n",
    "\\sum_{n=1}^N \\ln\\mathcal{N}(t_n|\\mathbf{w}^{\\text{T}}\\boldsymbol\\phi(\\mathbf{x}_n),\\beta^{-1}) \\\\\n",
    "&= \\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln 2\\pi - \n",
    "\\frac{\\beta}{2}\\sum_{n=1}^N \\big(t_n - \\mathbf{w}^{\\text{T}}\\boldsymbol\\phi(\\mathbf{x}_n)\\big)^2 \\\\\n",
    "&= \\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln 2\\pi - \\beta E_D(\\mathbf{w})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "By maximizing likelihood we can determine the parameters $\\mathbf{w}$ and $\\beta$. As already observed in [Chapter 1](ch1_introduction.ipynb) the maximization under a conditional Gaussian noise distribution is equivalent to minimizing the sum-of-squares error function given by $E_D(\\mathbf{w})$. The gradient of the log likelihood function takes the form\n",
    "\n",
    "$$\n",
    "\\nabla p(\\mathsf{t}|\\mathbf{X},\\mathbf{w},\\beta) = \n",
    "\\sum_{n=1}^N \\big(t_n - \\mathbf{w}^{\\text{T}}\\boldsymbol\\phi(\\mathbf{x}_n)\\big)\\phi(\\mathbf{x}_n)^{\\text{T}}\n",
    "$$\n",
    "\n",
    "Setting this gradient to zero and solving for $\\mathbf{w}$ gives\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{ML} = (\\mathbf{\\Phi}^{\\text{T}}\\mathbf{\\Phi})^{-1}\\mathbf{\\Phi}^{\\text{T}}\\mathsf{t}\n",
    "$$\n",
    "\n",
    "which are known as the normal equations for the least squares problem. Here $\\mathbf{\\Phi}$ is an $N\\times M$ matrix, called the *design matrix*, whose elements are given by $\\Phi_{nj} = \\phi_j(\\mathbf{x}_n)$, so that\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Phi}= \n",
    "\\begin{pmatrix}\n",
    "\\phi_0(\\mathbf{x}_1) & \\phi_1(\\mathbf{x}_1) & \\cdots & \\phi_{M-1}(\\mathbf{x}_1) \\\\\n",
    "\\phi_0(\\mathbf{x}_2) & \\phi_1(\\mathbf{x}_2) & \\cdots & \\phi_{M-1}(\\mathbf{x}_2) \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "\\phi_0(\\mathbf{x}_N) & \\phi_1(\\mathbf{x}_N) & \\cdots & \\phi_{M-1}(\\mathbf{x}_N)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "By maximizing the log likelihood function over the noise precision parameter $\\beta$, we obtain\n",
    "\n",
    "$$\n",
    "\\beta_{ML} = \\frac{1}{N}\\sum_{n=1}^N (t_n - \\mathbf{w}^{\\text{T}}\\boldsymbol\\phi(\\mathbf{x}_n))^2\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
